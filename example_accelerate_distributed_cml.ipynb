{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97044c1b-87ab-41fc-b459-84f8b14e0b1d",
   "metadata": {},
   "source": [
    "# Example Dsitributed accelerate on CML\n",
    "Using the huggingface accelerate API and CML Workers, show how to set up configurations to use multiple CML workers with GPU to perform distributed training.\n",
    "\n",
    "Training Script: https://github.com/huggingface/accelerate/blob/main/examples/nlp_example.py\n",
    "Script README: https://github.com/huggingface/accelerate/tree/main/examples#simple-nlp-example Simple NLP example\n",
    "->multi GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ef0a24-ba43-449f-98cf-e22a05062b0e",
   "metadata": {},
   "source": [
    "### Download training example from huggingface/accelerate git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c8a8bdb-e703-4108-86d8-8f7dfcc0c10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  8260  100  8260    0     0  41717      0 --:--:-- --:--:-- --:--:-- 41717\n"
     ]
    }
   ],
   "source": [
    "!curl https://raw.githubusercontent.com/huggingface/accelerate/main/examples/nlp_example.py -o nlp_example.py\n",
    "train_script = \"nlp_example.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b709d78-cedf-4c39-b275-419268ecd7d1",
   "metadata": {},
   "source": [
    "### pip install prereqs \n",
    "(for all imports used here or in the download training script example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "443ac8f0-3489-4686-9fcd-18e003a2eccf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./.local/lib/python3.9/site-packages (2.0.1)\n",
      "Requirement already satisfied: datasets in ./.local/lib/python3.9/site-packages (2.13.0)\n",
      "Requirement already satisfied: evaluate in ./.local/lib/python3.9/site-packages (0.4.0)\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.9/site-packages (4.31.0.dev0)\n",
      "Requirement already satisfied: accelerate in ./.local/lib/python3.9/site-packages (0.20.3)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in ./.local/lib/python3.9/site-packages (from torch) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in ./.local/lib/python3.9/site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in ./.local/lib/python3.9/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in ./.local/lib/python3.9/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/site-packages (from torch) (3.1.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./.local/lib/python3.9/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./.local/lib/python3.9/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in ./.local/lib/python3.9/site-packages (from torch) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./.local/lib/python3.9/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./.local/lib/python3.9/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in ./.local/lib/python3.9/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in ./.local/lib/python3.9/site-packages (from torch) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in ./.local/lib/python3.9/site-packages (from torch) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in ./.local/lib/python3.9/site-packages (from torch) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in ./.local/lib/python3.9/site-packages (from torch) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in ./.local/lib/python3.9/site-packages (from torch) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in ./.local/lib/python3.9/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (58.1.0)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.40.0)\n",
      "Requirement already satisfied: cmake in ./.local/lib/python3.9/site-packages (from triton==2.0.0->torch) (3.27.2)\n",
      "Requirement already satisfied: lit in ./.local/lib/python3.9/site-packages (from triton==2.0.0->torch) (16.0.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/site-packages (from datasets) (1.21.6)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in ./.local/lib/python3.9/site-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in ./.local/lib/python3.9/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.9/site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/site-packages (from datasets) (2.29.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in ./.local/lib/python3.9/site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in ./.local/lib/python3.9/site-packages (from datasets) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in ./.local/lib/python3.9/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in ./.local/lib/python3.9/site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in ./.local/lib/python3.9/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in ./.local/lib/python3.9/site-packages (from datasets) (0.16.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: responses<0.19 in ./.local/lib/python3.9/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.9/site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./.local/lib/python3.9/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./.local/lib/python3.9/site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: psutil in ./.local/lib/python3.9/site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: scipy>=1.5.0 in ./.local/lib/python3.9/site-packages (from scikit-learn) (1.11.1)\n",
      "Collecting joblib>=1.1.1 (from scikit-learn)\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/site-packages (from aiohttp->datasets) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.local/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./.local/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.local/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.local/lib/python3.9/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.local/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2020.11.8)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.local/lib/python3.9/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /runtime-addons/cmladdon-2.0.41-b121/opt/cmladdons/python/site-packages (from pandas->datasets) (2022.7.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.local/lib/python3.9/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.local/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.3.2 scikit-learn-1.3.0 threadpoolctl-3.2.0\n"
     ]
    }
   ],
   "source": [
    "! pip install torch datasets evaluate transformers accelerate scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337275b1-51c8-49a7-bec8-a3934303a367",
   "metadata": {},
   "source": [
    "### Generate accerlate config strings\n",
    "See https://huggingface.co/docs/accelerate/quicktour for guides on how to manually set up accelerate across workers if desired"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbd1e57-4c10-460e-b114-e214b0e51dcc",
   "metadata": {},
   "source": [
    "Generate config for:\n",
    "- NUM_WORKERS : number of separate CML sessions/workers to run\n",
    "- NUM_GPU_PER_WORKER : 1 GPU per CML Worker\n",
    "  - See gpu_ids in accelerate configuration guide to adjust this in your accelerate config template\n",
    "- MASTER_IP : The POD IP of this main CML session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20b42e40-b56f-4d4a-856d-789a18e5a54a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "NUM_WORKERS = 2\n",
    "NUM_GPU_PER_WORKER = 1\n",
    "MASTER_IP = os.environ[\"CDSW_IP_ADDRESS\"]\n",
    "\n",
    "\n",
    "conf_dir = \"./.tmp_accelerate_configs/\"\n",
    "config_path_tmpl = conf_dir + \"${WORKER}_config.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "100c6a32-1167-4f6b-8a74-af6cb83fdacf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating config 0\n",
      "creating config 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from string import Template\n",
    "\n",
    "template_file = open(\"accelerate_multi_config.yaml.tmpl\")\n",
    "template_string = template_file.read()\n",
    "template_file.close()\n",
    "\n",
    "os.makedirs(conf_dir, exist_ok=True)\n",
    "for i in range(NUM_WORKERS):\n",
    "    print(\"creating config %i\" % i)\n",
    "    config_file = Template(template_string)\n",
    "    config_file = config_file.substitute(MACHINE_RANK=i, MAIN_SESSION_IP=MASTER_IP, NUM_MACHINES=NUM_WORKERS, NUM_PROCESSES=NUM_WORKERS)\n",
    "    config_path = Template(config_path_tmpl).substitute(WORKER=i)\n",
    "\n",
    "    new_config = open(config_path, \"w\")\n",
    "    new_config.write(config_file)\n",
    "    new_config.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5be227-ce9c-499c-95e8-db08916fd482",
   "metadata": {},
   "source": [
    "### Execute accelerate CLI command on this session and spawned workers\n",
    "**Note:** This session counts as worker 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d02777a6-b222-4b3e-9580-eecd81cc7e12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Command template to launch accross all session/workers\n",
    "command_tmpl = \"accelerate launch --config_file $CONF_PATH $TRAIN_SCRIPT\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702a3f62-8029-4fdb-9ecd-7cf636a9d613",
   "metadata": {},
   "source": [
    "To launch accelerate training in distributed mode we need to execute accelerate launch as a shell command using specific config files for each \"accelerate worker\".\n",
    "\n",
    "If 2 \"accelerate workers\" are specified then there is a worker locally in this session and we launch an additional CML Worker\n",
    "\n",
    "If 3 \"accelerate workers\" are specified then there is a worker locally in this session and we launch two additional CML Worker and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e72303f-ba85-4ae5-9afc-cecf4ef3901e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launch accelerate locally (this session acts as worker of rank 1 aka main worker)...\n",
      "\t Command: [accelerate launch --config_file ./.tmp_accelerate_configs/0_config.yaml nlp_example.py]\n",
      "Launch CML worker and launch accelerate within them ...\n",
      "\t Command: [accelerate launch --config_file ./.tmp_accelerate_configs/1_config.yaml nlp_example.py]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 29.0/29.0 [00:00<00:00, 3.17kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 570/570 [00:00<00:00, 84.5kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 213k/213k [00:00<00:00, 8.51MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 436k/436k [00:00<00:00, 2.17MB/s]\n",
      "Downloading builder script: 100%|██████████| 28.8k/28.8k [00:00<00:00, 21.9MB/s]\n",
      "Downloading metadata: 100%|██████████| 28.7k/28.7k [00:00<00:00, 21.5MB/s]\n",
      "Downloading readme: 100%|██████████| 27.9k/27.9k [00:00<00:00, 23.8MB/s]\n",
      "Found cached dataset glue (/home/cdsw/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|██████████| 3/3 [00:00<00:00, 72.75it/s]\n",
      "Downloading model.safetensors: 100%|██████████| 436M/436M [00:01<00:00, 286MB/s]  \n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: {'accuracy': 0.7352941176470589, 'f1': 0.8373493975903614}\n",
      "epoch 1: {'accuracy': 0.7965686274509803, 'f1': 0.8676236044657097}\n",
      "epoch 2: {'accuracy': 0.8382352941176471, 'f1': 0.8846153846153846}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cml.workers_v1 import launch_workers\n",
    "import subprocess\n",
    "\n",
    "\n",
    "# Picking CPU and MEM profile\n",
    "worker_cpu = 2\n",
    "worker_memory = 8\n",
    "\n",
    "# if changing worker_gpu here, also change gpu_ids in accelerate_multi_config.yaml.tmpl\n",
    "worker_gpu = 1\n",
    "\n",
    "for i in range(NUM_WORKERS):\n",
    "    # Each accelerate launch requires different configuration\n",
    "    config_path = Template(config_path_tmpl).substitute(WORKER=i)\n",
    "    \n",
    "    # See top of notebook for where train_script comes from\n",
    "    command = Template(command_tmpl).substitute(CONF_PATH=config_path, TRAIN_SCRIPT=train_script)\n",
    "\n",
    "    # Wrapping execution into subprocess for convenience in this notebook, but this could be done manually or via CML Jobs\n",
    "    # If worker num 0 this is the main process and should run locally in this session\n",
    "    if i == 0:\n",
    "        print(\"Launch accelerate locally (this session acts as worker of rank 1 aka main worker)...\")\n",
    "        print(\"\\t Command: [%s]\" % command)\n",
    "        main_cmd = subprocess.Popen([f'bash -c \"{command}\" '], shell=True)\n",
    "\n",
    "    # All other accelerate launches will use rank 1+\n",
    "    else:\n",
    "        print((\"Launch CML worker and launch accelerate within them ...\"))\n",
    "        print(\"\\t Command: [%s]\" % command)\n",
    "        launch_workers(name=f'LoRA Train Worker {i}', n=1, cpu=worker_cpu, memory=worker_memory, nvidia_gpu = worker_gpu,  code=\"!\"+command + \" &> /dev/null\")\n",
    "\n",
    "# Waiting for all subworkers to ready up...\n",
    "main_cmd.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6045d4-98d8-4ec5-b389-d3b28fed4da8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
