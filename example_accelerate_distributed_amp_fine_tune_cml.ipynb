{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97044c1b-87ab-41fc-b459-84f8b14e0b1d",
   "metadata": {},
   "source": [
    "# Accelerate Distributed Fine-Tuning a foundation model for multiple tasks (with QLoRA)\n",
    "Using the huggingface accelerate API and CML Workers, show how to set up configurations to use multiple CML workers with GPU to perform distributed training.\n",
    "\n",
    "The following notebook is an example of performing the bundled QLoRA fine-tuning on an LLM using an instruction-following dataset distributed across multiple CML Workers. This script produces the same instruction-following adapter as shown in the amp_adapters_prebuilt directory and the CML Job \"Job for fine-tuning on Instruction Dataset\"\n",
    "\n",
    "Requirements:\n",
    "- Notebook Session:\n",
    "  - 2 CPU / 8 MEM / 1 GPU\n",
    "- GPUs:\n",
    "This notebook requires access within this CML workspace for a total of 2 GPUs.\n",
    "  - 1 for this Notebook Session (described above)\n",
    "  - 1 for the spawned CML Worker.\n",
    "- Runtime:\n",
    "  - JupyterLab - Python 3.9 - Nvidia GPU - 2023.05\n",
    "\n",
    "Note: This executes fine-tuning code defined in fine_tune_src/distributed_peft_scripts. See the implementation README in fine_tune_src/distributed_peft_scripts for a description of the fine-tuning code using huggingface transformers/trl."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ef0a24-ba43-449f-98cf-e22a05062b0e",
   "metadata": {},
   "source": [
    "### Set Training Script Path\n",
    "This is the training script that will be distributed. The script itself can be run standalone or distributed with accelerate thanks to huggingface transformer and trl integration with accelerate internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c8a8bdb-e703-4108-86d8-8f7dfcc0c10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_script = \"fine_tune_src/distributed_peft_scripts/task_instruction_fine_tuner.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b709d78-cedf-4c39-b275-419268ecd7d1",
   "metadata": {},
   "source": [
    "## Part 0: Install Dependencies\n",
    "\n",
    "Install dependencies for all imports used in this notebook or referenced in the distributed fine-tuning script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "443ac8f0-3489-4686-9fcd-18e003a2eccf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q --no-cache-dir -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337275b1-51c8-49a7-bec8-a3934303a367",
   "metadata": {},
   "source": [
    "## Part 1: Generate accelerate configuration\n",
    "See https://huggingface.co/docs/accelerate/quicktour for guides on how to manually set up accelerate across workers if desired"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbd1e57-4c10-460e-b114-e214b0e51dcc",
   "metadata": {},
   "source": [
    "Must generate configurations for:\n",
    "- NUM_WORKERS : (2) number of separate CML sessions/workers to run\n",
    "- NUM_GPU_PER_WORKER : (1) GPU per CML Worker\n",
    "  - See gpu_ids in accelerate configuration guide to adjust this in your accelerate config template\n",
    "- MASTER_IP : The POD IP of this main CML session\n",
    "\n",
    "These are the main variable configurations for accelerate we are concerned with to control distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20b42e40-b56f-4d4a-856d-789a18e5a54a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "NUM_WORKERS = 3\n",
    "NUM_GPU_PER_WORKER = 1\n",
    "MASTER_IP = os.environ[\"CDSW_IP_ADDRESS\"]\n",
    "\n",
    "# Set directory for all sub-workers to pull configurations from\n",
    "conf_dir = \"./.tmp_accelerate_configs_notebook/\"\n",
    "config_path_tmpl = conf_dir + \"${WORKER}_config.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc39531",
   "metadata": {},
   "source": [
    "Different accelerate configurations are required for each accelerate worker, set that up here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "100c6a32-1167-4f6b-8a74-af6cb83fdacf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating config 0\n",
      "creating config 1\n",
      "creating config 2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from string import Template\n",
    "\n",
    "template_file = open(\"fine_tune_src/distributed_peft_scripts/common/accelerate_configs/accelerate_multi_config.yaml.tmpl\")\n",
    "template_string = template_file.read()\n",
    "template_file.close()\n",
    "\n",
    "os.makedirs(conf_dir, exist_ok=True)\n",
    "for i in range(NUM_WORKERS):\n",
    "    print(\"creating config %i\" % i)\n",
    "    config_file = Template(template_string)\n",
    "    config_file = config_file.substitute(MACHINE_RANK=i, MAIN_SESSION_IP=MASTER_IP, NUM_MACHINES=NUM_WORKERS, NUM_PROCESSES=NUM_WORKERS)\n",
    "    config_path = Template(config_path_tmpl).substitute(WORKER=i)\n",
    "\n",
    "    new_config = open(config_path, \"w\")\n",
    "    new_config.write(config_file)\n",
    "    new_config.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5be227-ce9c-499c-95e8-db08916fd482",
   "metadata": {},
   "source": [
    "## Part 2: Execute accelerate CLI command on this session and spawned workers\n",
    "**Note:** This session counts as worker 0\n",
    "\n",
    "Using the predefined fine-tuning script, launch distributed fine-tuning by launching accelerate on CML Workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d02777a6-b222-4b3e-9580-eecd81cc7e12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Command template to launch accross all session/workers\n",
    "command_tmpl = \"accelerate launch --config_file $CONF_PATH $TRAIN_SCRIPT\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702a3f62-8029-4fdb-9ecd-7cf636a9d613",
   "metadata": {},
   "source": [
    "To launch accelerate training in distributed mode we need to execute accelerate launch as a shell command using specific config files for each \"accelerate worker\".\n",
    "\n",
    "eg. If 2 \"accelerate workers\" are specified then there is a worker locally in this session and we launch an additional CML Worker\n",
    "\n",
    "eg. If 3 \"accelerate workers\" are specified then there is a worker locally in this session and we launch two additional CML Worker and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e72303f-ba85-4ae5-9afc-cecf4ef3901e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launch accelerate locally (this session acts as worker of rank 1 aka main worker)...\n",
      "\t Command: [accelerate launch --config_file ./.tmp_accelerate_configs_notebook/0_config.yaml fine_tune_src/distributed_peft_scripts/task_instruction_fine_tuner.py]\n",
      "Launch CML worker and launch accelerate within them ...\n",
      "\t Command: [accelerate launch --config_file ./.tmp_accelerate_configs_notebook/1_config.yaml fine_tune_src/distributed_peft_scripts/task_instruction_fine_tuner.py]\n",
      "Launch CML worker and launch accelerate within them ...\n",
      "\t Command: [accelerate launch --config_file ./.tmp_accelerate_configs_notebook/2_config.yaml fine_tune_src/distributed_peft_scripts/task_instruction_fine_tuner.py]\n",
      "bin /home/cdsw/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
      "Downloading and preparing dataset json/teknium--GPTeacher-General-Instruct to /home/cdsw/.cache/oinopwujnkn5g05p/huggingface/datasets/teknium___json/teknium--GPTeacher-General-Instruct-3d3eb51407944fd2/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|          | 0.00/12.3M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data: 100%|██████████| 12.3M/12.3M [00:00<00:00, 109MB/s]\u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/12.1M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data: 100%|██████████| 12.1M/12.1M [00:00<00:00, 115MB/s]\u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/12.2M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data: 100%|██████████| 12.2M/12.2M [00:00<00:00, 113MB/s]\u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/12.2M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data: 100%|██████████| 12.2M/12.2M [00:00<00:00, 107MB/s]\u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/12.2M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data: 100%|██████████| 12.2M/12.2M [00:00<00:00, 121MB/s]\u001b[A\n",
      "Downloading data files: 100%|██████████| 1/1 [00:02<00:00,  2.40s/it]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 17.15it/s]\n",
      "Map:   0%|          | 0/26778 [00:00<?, ? examples/s]              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/cdsw/.cache/oinopwujnkn5g05p/huggingface/datasets/teknium___json/teknium--GPTeacher-General-Instruct-3d3eb51407944fd2/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the base model and tokenizer...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdsw/.local/lib/python3.9/site-packages/transformers/modeling_utils.py:2192: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 385505280 || all params: 725575680 || trainable%: 53.1309538930522\n",
      "Begin Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5563, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.4127, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.3082, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.3514, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.182, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.3055, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.3212, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.2462, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.2382, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.1217, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.1829, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.0968, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.0945, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.1266, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.0295, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.1909, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.1161, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.0723, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.062, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.0236, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.9947, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.0352, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.077, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.0853, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.1022, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.0462, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.0657, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.056, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.0889, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.1129, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.015, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.0337, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.0082, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.0637, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.9879, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 2.0433, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 2.0197, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 2.0307, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 2.0286, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 2.0839, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.9931, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 2.0105, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 2.0323, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 2.0483, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 2.0117, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 2.0268, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.9988, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.9813, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.9854, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 2.0196, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.9192, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 2.0004, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.9867, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 2.0558, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.8681, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 2.0457, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 2.0002, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 2.0231, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 2.0762, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9872, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9488, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 2.0337, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9121, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.905, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 2.0057, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9843, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 2.0312, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 2.0384, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 2.0674, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 2.0564, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9944, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 2.0366, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 2.0687, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.992, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 2.0802, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9779, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 2.0029, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9973, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 2.0453, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.0068, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.0494, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.0156, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.0133, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.0658, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.0695, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.9721, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.1235, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.9911, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.0628, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.0515, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.0549, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.9707, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.0405, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.0074, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.9876, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.988, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.0556, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.0026, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.0492, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.9887, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.0017, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 2.0241, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9993, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 2.0053, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9631, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 2.0082, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 2.0097, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9741, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 2.0286, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 2.0482, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9405, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 2.0396, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 2.0855, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9926, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9845, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9771, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9683, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9585, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9952, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 2.0071, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9558, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 2.0366, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 2.0255, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9753, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9597, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9259, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9502, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 2.0166, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 2.0938, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9916, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9975, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9424, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 2.001, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9484, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 2.0113, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9935, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 2.0142, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 2.0098, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9339, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.955, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 2.0194, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.935, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9878, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9876, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 2.0208, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 2.048, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.9548, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 2.0592, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 2.0529, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 2.0095, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 2.0648, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 2.0284, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.9617, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.9583, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.9806, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.9726, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 2.0006, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.9913, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 2.0281, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 2.084, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.9725, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 2.0041, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 2.023, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.9682, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 2.0005, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.96, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 2.0013, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 2.0298, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 2.0033, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 2.0719, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.9912, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 2.0574, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.9688, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.97, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.9316, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.9477, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.9294, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 2.0297, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 2.045, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.9356, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 2.0187, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.9707, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.9963, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 2.0395, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.9352, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.9811, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.9977, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.9203, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.9941, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.9745, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 2.0015, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.8437, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.9139, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.9477, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.877, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.8924, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.9015, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.9197, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.8834, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.8179, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.8486, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.873, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.8825, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.9175, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.8079, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.9213, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.8242, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.8908, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.896, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.8764, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.8767, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.8304, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.8773, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.8951, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.9168, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.8964, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.8717, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.9412, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.9364, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.8489, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.9683, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.9, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.9614, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.9025, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.8109, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.8704, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.9478, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.8662, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.9685, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.8205, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.8537, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.9485, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.9257, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.8677, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.8457, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.9603, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.9264, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.822, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.9422, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.913, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.826, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.9194, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.8664, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.9074, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.8178, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.9886, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.8379, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.875, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.8055, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.9198, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.8691, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.7804, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.8507, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.8933, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.9627, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.9366, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.9425, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.9052, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.8888, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.8882, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.9341, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.9132, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.9631, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.8816, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.8725, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.9583, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.963, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.9303, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.8743, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.9667, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.9324, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.9013, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.9647, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.9169, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.9197, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.9756, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.9043, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.8885, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.9612, 'learning_rate': 0.0002, 'epoch': 0.13}\n",
      "{'loss': 1.9151, 'learning_rate': 0.0002, 'epoch': 0.13}\n",
      "{'loss': 1.8762, 'learning_rate': 0.0002, 'epoch': 0.13}\n",
      "{'loss': 1.9381, 'learning_rate': 0.0002, 'epoch': 0.13}\n",
      "{'train_runtime': 802.7101, 'train_samples_per_second': 33.359, 'train_steps_per_second': 2.779, 'train_loss': 1.990915373284766, 'epoch': 0.13}\n",
      "Training Complete!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    # Launch workers when using CML\n",
    "    from cml.workers_v1 import launch_workers\n",
    "except ImportError:\n",
    "    # Launch workers when using CDSW\n",
    "    from cdsw import launch_workers\n",
    "import subprocess\n",
    "\n",
    "\n",
    "# Picking CPU and MEM profile\n",
    "worker_cpu = 2\n",
    "worker_memory = 8\n",
    "\n",
    "# if changing worker_gpu here, also change gpu_ids in accelerate_multi_config.yaml.tmpl\n",
    "worker_gpu = 1\n",
    "\n",
    "for i in range(NUM_WORKERS):\n",
    "    # Each accelerate launch requires different configuration\n",
    "    config_path = Template(config_path_tmpl).substitute(WORKER=i)\n",
    "    \n",
    "    # See top of notebook for where train_script comes from\n",
    "    command = Template(command_tmpl).substitute(CONF_PATH=config_path, TRAIN_SCRIPT=train_script)\n",
    "\n",
    "    # Wrapping execution into subprocess for convenience in this notebook, but this could be done manually or via CML Jobs\n",
    "    # If worker num 0 this is the main process and should run locally in this session\n",
    "    if i == 0:\n",
    "        print(\"Launch accelerate locally (this session acts as worker of rank 1 aka main worker)...\")\n",
    "        print(\"\\t Command: [%s]\" % command)\n",
    "        main_cmd = subprocess.Popen([f'bash -c \"{command}\" '], shell=True)\n",
    "\n",
    "    # All other accelerate launches will use rank 1+\n",
    "    else:\n",
    "        print((\"Launch CML worker and launch accelerate within them ...\"))\n",
    "        print(\"\\t Command: [%s]\" % command)\n",
    "        launch_workers(name=f'LoRA Train Worker {i}', n=1, cpu=worker_cpu, memory=worker_memory, nvidia_gpu = worker_gpu,  code=\"!\"+command + \" &> /dev/null\")\n",
    "\n",
    "# Waiting for all subworkers to ready up...\n",
    "main_cmd.communicate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12a7904",
   "metadata": {},
   "source": [
    "## Done!\n",
    "Your fine-tuned adapter is located in /home/cdsw/adapters/bloom1b1-lora-instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db84cda",
   "metadata": {},
   "source": [
    "## Part 3: Inference Comparison (Base Model vs Base Model + Adapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6cc587",
   "metadata": {},
   "source": [
    "### Load base model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9091a90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdsw/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/cdsw/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /home/cdsw/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdsw/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib'), PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-1b1\", return_dict=True, device_map='cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-1b1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd3cc92",
   "metadata": {},
   "source": [
    "### Load the fine-tuned adapter for use with the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5853d74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = PeftModel.from_pretrained(model=model,                                                 # The base model to load fine-tuned adapters with\n",
    "                                  model_id=\"/home/cdsw/adapters/bloom1b1-lora-instruct\",       # The directory path of the fine-tuned adapater built in Part 1\n",
    "                                  adapter_name=\"bloom1b1-lora-instruct\",              # A label for this adapter to enable and disable on demand later\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449dbde0",
   "metadata": {},
   "source": [
    "### Define an instruction-following test prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57498fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"<Instruction>: Classify the following items into two categories: fruits and vegetables.\n",
    "<Input>: tomato, apple, cucumber, carrot, banana, zucchini, strawberry, cauliflower\n",
    "<Response>:\"\"\"\n",
    "batch = tokenizer(prompt, return_tensors='pt')\n",
    "batch = batch.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27b625a",
   "metadata": {},
   "source": [
    "#### Base Model Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b9df26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " green, yellow, red, orange, red, yellow, green, blue, yellow, red, orange, red, yellow, green, blue, yellow, red, orange, red, yellow, green, blue, yellow, red, orange, red, yellow, green, blue, yellow,\n"
     ]
    }
   ],
   "source": [
    "# Inference with base model only:\n",
    "import torch\n",
    "with model.disable_adapter():\n",
    "    with torch.cuda.amp.autocast():\n",
    "        output_tokens = model.generate(**batch, max_new_tokens=60)\n",
    "    prompt_length = len(prompt)\n",
    "    print(tokenizer.decode(output_tokens[0], skip_special_tokens=True)[prompt_length:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716c588e",
   "metadata": {},
   "source": [
    "^ The base model shows no ability to follow instructions in the promp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc78eb9",
   "metadata": {},
   "source": [
    "#### Fine-tuned adapter Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cbeff01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Fruits: Tomato, Apple, Cucumber, Carrot, Banana, Zucchini, Strawberry, Cauliflower. Vegetables: Tomato, Apple, Cucumber, Carrot, Banana, Zucchini, Strawberry, Cauliflower\n"
     ]
    }
   ],
   "source": [
    "# Inference with fine-tuned adapter:\n",
    "model.set_adapter(\"bloom1b1-lora-instruct\")\n",
    "with torch.cuda.amp.autocast():\n",
    "    output_tokens = model.generate(**batch, max_new_tokens=60)\n",
    "prompt_length = len(prompt)\n",
    "print(tokenizer.decode(output_tokens[0], skip_special_tokens=True)[prompt_length:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5b7ffd",
   "metadata": {},
   "source": [
    "^ This is not a perfect response, but a good step towards a usable instruction-following LLM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
